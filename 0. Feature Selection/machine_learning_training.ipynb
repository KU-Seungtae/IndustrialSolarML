{"cells":[{"cell_type":"markdown","metadata":{"id":"JSnaHs302Gbw"},"source":["# Machine Learning Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hp7DQI4p2Gbz"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import warnings\n","import joblib\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n","\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.ensemble import (\n","    RandomForestRegressor,\n","    GradientBoostingRegressor,\n","    ExtraTreesRegressor,\n","    AdaBoostRegressor\n",")\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.linear_model import Ridge, LinearRegression\n","\n","# Configure environment\n","warnings.filterwarnings(\"ignore\")\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","\n","# Set random seed\n","RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","\n","# Load dataset\n","file_path = os.path.join(os.getcwd(), \"multiwafer_database_all_feature.xlsx\")\n","data = pd.read_excel(file_path)\n","\n","# Define input and output features\n","all_features = [\n","    'P01', 'P02', 'P03', 'P04', 'P05', 'P06', 'Tester', 'P03 Recipe', 'P06 Recipe',\n","    'Paste 1', 'Paste 2', 'Dark Area %', 'Defect Area %', 'Grain Defect Area %',\n","    'Average Life Time', 'Sigma Life Time', 'Resistivity', 'Bow', 'Sawmark',\n","    'Avg THK', 'TTV', 'WARP', 'Wafer Area', 'Vendor name'\n","]\n","X = data[all_features]\n","X.columns = X.columns.str.replace(\" \", \"_\")\n","target_features = [\"Efficiency\"]\n","\n","# Create folder for Optuna logs\n","os.makedirs(\"OptimizationLogs\", exist_ok=True)\n","\n","# Model definition and search spaces\n","model_dict = {\n","    \"XGB\": (XGBRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"max_depth\": (5, 30),\n","        \"learning_rate\": (0.001, 0.3),\n","        \"colsample_bytree\": (0.3, 1.0),\n","        \"subsample\": (0.5, 1.0)\n","    }),\n","    \"RF\": (RandomForestRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"max_depth\": (5, 30),\n","        \"min_samples_split\": (2, 20),\n","        \"min_samples_leaf\": (1, 10)\n","    }),\n","    \"GBR\": (GradientBoostingRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"max_depth\": (5, 30),\n","        \"learning_rate\": (0.001, 0.3),\n","        \"subsample\": (0.5, 1.0)\n","    }),\n","    \"LGB\": (LGBMRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"max_depth\": (5, 30),\n","        \"learning_rate\": (0.001, 0.3),\n","        \"subsample\": (0.5, 1.0),\n","        \"colsample_bytree\": (0.5, 1.0)\n","    }),\n","    \"ADA\": (AdaBoostRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"learning_rate\": (0.01, 1.0)\n","    }),\n","    \"DT\": (DecisionTreeRegressor, {\n","        \"max_depth\": (5, 30),\n","        \"min_samples_split\": (2, 20),\n","        \"min_samples_leaf\": (1, 10)\n","    }),\n","    \"ET\": (ExtraTreesRegressor, {\n","        \"n_estimators\": (50, 500),\n","        \"max_depth\": (5, 30),\n","        \"min_samples_split\": (2, 20),\n","        \"min_samples_leaf\": (1, 10)\n","    }),\n","    \"Ridge\": (Ridge, {\n","        \"alpha\": (1e-4, 10.0)\n","    }),\n","}\n","\n","results = []\n","\n","# Loop over each output feature\n","for target in target_features:\n","    y = data[target]\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE)\n","\n","    for model_name, (model_class, param_dict) in model_dict.items():\n","\n","        def objective(trial):\n","            params = {}\n","            for key, value in param_dict.items():\n","                if isinstance(value[0], int):\n","                    params[key] = trial.suggest_int(key, value[0], value[1])\n","                else:\n","                    params[key] = trial.suggest_float(key, value[0], value[1], log=True)\n","            params[\"random_state\"] = RANDOM_STATE\n","            if \"n_jobs\" in model_class().get_params():\n","                params[\"n_jobs\"] = -1\n","            if model_name == \"XGB\":\n","                params[\"tree_method\"] = \"hist\"\n","                params[\"device\"] = \"cuda\"\n","            if model_name == \"LGB\":\n","                params[\"verbose\"] = -1\n","            model = model_class(**params)\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_val)\n","            return mean_absolute_percentage_error(y_val, y_pred) * 100\n","\n","        study = optuna.create_study(direction=\"minimize\")\n","        study.optimize(objective, n_trials=50)\n","\n","        # Log trial history\n","        trial_data = [{**trial.params, \"MAPE\": trial.value} for trial in study.trials]\n","        df_trials = pd.DataFrame(trial_data)\n","        df_trials.to_excel(f\"OptimizationLogs/OptimizationLog_{target}_{model_name}.xlsx\", index=False)\n","\n","        # Final training using train + val set\n","        X_final_train = pd.concat([X_train, X_val])\n","        y_final_train = pd.concat([y_train, y_val])\n","\n","        best_params = study.best_params\n","        best_params[\"random_state\"] = RANDOM_STATE\n","        if \"n_jobs\" in model_class().get_params():\n","            best_params[\"n_jobs\"] = -1\n","        if model_name == \"XGB\":\n","            best_params[\"tree_method\"] = \"hist\"\n","            best_params[\"device\"] = \"cuda\"\n","        if model_name == \"LGB\":\n","            best_params[\"verbose\"] = -1\n","\n","        final_model = model_class(**best_params)\n","        final_model.fit(X_final_train, y_final_train)\n","        y_pred = final_model.predict(X_test)\n","\n","        joblib.dump(final_model, f\"({target})best_model_{model_name}.pkl\")\n","\n","        results.append({\n","            \"Output\": target,\n","            \"Model\": model_name,\n","            \"R2\": r2_score(y_test, y_pred),\n","            \"MAPE\": mean_absolute_percentage_error(y_test, y_pred) * 100,\n","            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred))\n","        })\n","\n","    # Linear regression baseline\n","    lr_model = LinearRegression()\n","    lr_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n","    y_pred_lr = lr_model.predict(X_test)\n","    joblib.dump(lr_model, f\"({target})best_model_LR.pkl\")\n","    results.append({\n","        \"Output\": target,\n","        \"Model\": \"LR\",\n","        \"R2\": r2_score(y_test, y_pred_lr),\n","        \"MAPE\": mean_absolute_percentage_error(y_test, y_pred_lr) * 100,\n","        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_lr))\n","    })\n","\n","# Save final model performance results\n","results_df = pd.DataFrame(results)\n","results_df.to_excel(\"All_Models_result.xlsx\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"YMIZ8d6C2Gb0"},"source":["# Deep Learning Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeD627f-2Gb1"},"outputs":[],"source":["import os\n","import random\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import joblib\n","import warnings\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n","\n","# Reproducibility\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","file_path = os.path.join(os.getcwd(), \"multiwafer_database_all_feature.xlsx\")\n","data = pd.read_excel(file_path)\n","\n","# Features and multiple targets\n","all_features = [\n","    'P01', 'P02', 'P03', 'P04', 'P05', 'P06', 'Tester', 'P03 Recipe', 'P06 Recipe',\n","    'Paste 1', 'Paste 2', 'Dark Area %', 'Defect Area %', 'Grain Defect Area %',\n","    'Average Life Time', 'Sigma Life Time', 'Resistivity', 'Bow', 'Sawmark',\n","    'Avg THK', 'TTV', 'WARP', 'Wafer Area', 'Vendor name'\n","]\n","output_features = [\"Efficiency\"]\n","\n","X = data[all_features]\n","X.columns = X.columns.str.replace(\" \", \"_\")\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","# Model definition\n","class MLPModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dims, dropout):\n","        super(MLPModel, self).__init__()\n","        layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout)]\n","        for i in range(1, len(hidden_dims)):\n","            layers.extend([\n","                nn.Linear(hidden_dims[i - 1], hidden_dims[i]),\n","                nn.ReLU(),\n","                nn.Dropout(dropout)\n","            ])\n","        layers.append(nn.Linear(hidden_dims[-1], 1))\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Final results and logs\n","final_all_results = []\n","trial_all_logs = []\n","\n","# Loop over each output variable\n","for output_feature in output_features:\n","    y = data[output_feature]\n","    scaler_y = StandardScaler()\n","    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n","\n","    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y_scaled, test_size=0.4, random_state=42)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n","    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n","    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n","    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n","\n","    def objective_mlp(trial):\n","        num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n","        hidden_dims = [trial.suggest_int(f\"hidden_dim_{i+1}\", 16, 512) for i in range(num_hidden_layers)]\n","        dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n","        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n","        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n","\n","        model = MLPModel(input_dim=X_train.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.MSELoss()\n","\n","        train_loader = torch.utils.data.DataLoader(\n","            list(zip(X_train_tensor, y_train_tensor)), batch_size=batch_size, shuffle=True\n","        )\n","\n","        best_val_loss = float(\"inf\")\n","        no_improve_count = 0\n","        patience = 30\n","\n","        for epoch in range(500):\n","            model.train()\n","            for batch_X, batch_y in train_loader:\n","                optimizer.zero_grad()\n","                y_pred = model(batch_X)\n","                loss = criterion(y_pred, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","            model.eval()\n","            with torch.no_grad():\n","                val_pred = model(X_val_tensor)\n","                val_loss = criterion(val_pred, y_val_tensor).item()\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                no_improve_count = 0\n","            else:\n","                no_improve_count += 1\n","                if no_improve_count >= patience:\n","                    break\n","\n","        with torch.no_grad():\n","            y_pred_val = model(X_val_tensor).cpu().numpy().flatten()\n","        return mean_absolute_percentage_error(y_val, y_pred_val) * 100\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(objective_mlp, n_trials=100)\n","\n","    top_trials = sorted(study.trials, key=lambda x: x.value)[:5]\n","\n","    for idx, trial in enumerate(top_trials):\n","        params = trial.params\n","        hidden_dims = [params[f\"hidden_dim_{i+1}\"] for i in range(params[\"num_hidden_layers\"])]\n","\n","        model = MLPModel(X_train.shape[1], hidden_dims, params[\"dropout\"]).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n","        criterion = nn.MSELoss()\n","        batch_size = params[\"batch_size\"]\n","\n","        train_loader = torch.utils.data.DataLoader(\n","            list(zip(X_train_tensor, y_train_tensor)), batch_size=batch_size, shuffle=True\n","        )\n","\n","        best_val_loss = float(\"inf\")\n","        no_improve_count = 0\n","        patience = 30\n","\n","        for epoch in range(500):\n","            model.train()\n","            for batch_X, batch_y in train_loader:\n","                optimizer.zero_grad()\n","                y_pred = model(batch_X)\n","                loss = criterion(y_pred, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","            model.eval()\n","            with torch.no_grad():\n","                val_pred = model(X_val_tensor)\n","                val_loss = criterion(val_pred, y_val_tensor).item()\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                no_improve_count = 0\n","            else:\n","                no_improve_count += 1\n","                if no_improve_count >= patience:\n","                    break\n","\n","        model.eval()\n","        with torch.no_grad():\n","            y_pred_test_scaled = model(X_test_tensor).cpu().numpy()\n","        y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n","        y_true_test = scaler_y.inverse_transform(y_test_tensor.cpu().numpy())\n","\n","        r2 = r2_score(y_true_test, y_pred_test)\n","        mape = mean_absolute_percentage_error(y_true_test, y_pred_test) * 100\n","        rmse = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n","\n","        final_all_results.append({\n","            \"Output\": output_feature,\n","            \"Trial\": idx,\n","            \"R2\": r2,\n","            \"MAPE\": mape,\n","            \"RMSE\": rmse\n","        })\n","        trial_all_logs.append({\n","            \"Output\": output_feature,\n","            \"Trial\": idx,\n","            \"MAPE\": mape,\n","            \"Hyperparameters\": params\n","        })\n","\n","        if idx == 0:\n","            torch.save(model.state_dict(), f\"best_model_MLP_{output_feature}.pth\")\n","\n","            # Final training with train + validation data\n","            X_final_train = np.vstack([X_train, X_val])\n","            y_final_train = np.concatenate([y_train, y_val])\n","            X_final_train_tensor = torch.tensor(X_final_train, dtype=torch.float32).to(device)\n","            y_final_train_tensor = torch.tensor(y_final_train, dtype=torch.float32).view(-1, 1).to(device)\n","\n","            final_model = MLPModel(X_final_train.shape[1], hidden_dims, params[\"dropout\"]).to(device)\n","            final_optimizer = optim.Adam(final_model.parameters(), lr=params[\"learning_rate\"])\n","            final_criterion = nn.MSELoss()\n","            final_train_loader = torch.utils.data.DataLoader(\n","                list(zip(X_final_train_tensor, y_final_train_tensor)),\n","                batch_size=batch_size, shuffle=True\n","            )\n","\n","            best_loss = float(\"inf\")\n","            no_improve_count = 0\n","            for epoch in range(500):\n","                final_model.train()\n","                for batch_X, batch_y in final_train_loader:\n","                    final_optimizer.zero_grad()\n","                    y_pred = final_model(batch_X)\n","                    loss = final_criterion(y_pred, batch_y)\n","                    loss.backward()\n","                    final_optimizer.step()\n","\n","                final_model.eval()\n","                with torch.no_grad():\n","                    val_loss = final_criterion(final_model(X_final_train_tensor), y_final_train_tensor).item()\n","                if val_loss < best_loss:\n","                    best_loss = val_loss\n","                    no_improve_count = 0\n","                else:\n","                    no_improve_count += 1\n","                    if no_improve_count >= patience:\n","                        break\n","\n","            final_model.eval()\n","            with torch.no_grad():\n","                y_pred_test_final_scaled = final_model(X_test_tensor).cpu().numpy()\n","            y_pred_test_final = scaler_y.inverse_transform(y_pred_test_final_scaled)\n","            y_true_test_final = scaler_y.inverse_transform(y_test_tensor.cpu().numpy())\n","\n","            r2_final = r2_score(y_true_test_final, y_pred_test_final)\n","            mape_final = mean_absolute_percentage_error(y_true_test_final, y_pred_test_final) * 100\n","            rmse_final = np.sqrt(mean_squared_error(y_true_test_final, y_pred_test_final))\n","\n","            final_all_results.append({\n","                \"Output\": output_feature,\n","                \"Trial\": \"Best (Retrain)\",\n","                \"R2\": r2_final,\n","                \"MAPE\": mape_final,\n","                \"RMSE\": rmse_final\n","            })\n","            torch.save(final_model.state_dict(), f\"best_model_MLP_{output_feature}_final.pth\")\n","\n","# Save overall performance results to Excel\n","results_df = pd.DataFrame(final_all_results)\n","results_df.to_excel(\"final_results_MLP_all.xlsx\", index=False)\n","\n","# Save individual trial hyperparameters and performance\n","logs_df = pd.DataFrame(trial_all_logs)\n","logs_df.to_excel(\"trial_logs_MLP_all.xlsx\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"GyjB8NOj2Gb1"},"source":["# Feature importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHJaq3Ym2Gb1","outputId":"94d38e88-e5bc-4b19-a801-88e9eb07a18e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Efficiency feature importance saved.\n"]}],"source":["import os\n","import joblib\n","import pandas as pd\n","\n","# Output directory\n","output_dir = \"SHAP\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Load dataset\n","data = pd.read_excel(\"multiwafer_database_all_feature.xlsx\")\n","data.columns = data.columns.str.replace(\" \", \"_\")\n","\n","# Input features (updated to match column names after underscore replacement)\n","all_features = [\n","    'P01', 'P02', 'P03', 'P04', 'P05', 'P06', 'Tester', 'P03 Recipe', 'P06 Recipe',\n","    'Paste 1', 'Paste 2', 'Dark Area %', 'Defect Area %', 'Grain Defect Area %',\n","    'Average Life Time', 'Sigma Life Time', 'Resistivity', 'Bow', 'Sawmark',\n","    'Avg THK', 'TTV', 'WARP', 'Wafer Area', 'Vendor name'\n","]\n","\n","# Select features\n","X = data[all_features]\n","\n","# Load trained model\n","model_path = \"./(Efficiency)best_model_ET.pkl\"\n","model = joblib.load(model_path)\n","\n","# Feature importance\n","importance = model.feature_importances_\n","df_importance = pd.DataFrame({\n","    \"Feature\": X.columns,\n","    \"Importance\": importance\n","}).sort_values(by=\"Importance\", ascending=False)\n","\n","# Save to Excel\n","df_importance.to_excel(os.path.join(output_dir, \"Efficiency_Feature_Importance.xlsx\"), index=False)\n","\n","print(\"Efficiency feature importance saved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qgnpOFz2Gb2"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"250605_multiwafer","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}